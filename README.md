
# ğŸ“Š Customer Churn Prediction â€“ End-to-End Machine Learning System

## ğŸ“Œ Overview

This project implements a **complete, production-oriented Customer Churn Prediction system**, covering the full machine learning lifecycle:

* Exploratory Data Analysis (EDA)
* Feature engineering & preprocessing
* Model training, tuning, and evaluation
* Model interpretability
* Real-time inference via FastAPI
* Prediction logging with SQLAlchemy
* Monitoring dashboard with Streamlit
* Model drift detection using PSI (Population Stability Index)

The goal of this project is not just to build an accurate churn model, but to demonstrate **how ML systems are designed, deployed, and monitored in real-world settings**.

---

## ğŸ§  Problem Statement

Customer churn is a critical business problem where organizations aim to identify customers likely to leave their service.
This project predicts whether a customer will churn based on demographic, contractual, and usage-related features, enabling proactive retention strategies.

---

## ğŸ—‚ï¸ Project Structure

```
customer_churn_prediction/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ customer_churn_model.ipynb      # EDA, feature engineering, modeling
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                          # FastAPI inference service
â”‚   â”œâ”€â”€ predict.py                       # Model inference logic
â”‚   â”œâ”€â”€ schemas.py                       # Request validation schemas
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ models.py                        # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ db.py                            # Database connection
â”‚   â”œâ”€â”€ logs.db                          # SQLite prediction logs
â”‚   â”œâ”€â”€ dashboard.py                    # Streamlit monitoring dashboard
â”‚             
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ model.pkl                        # Trained model
â”‚   â”œâ”€â”€ preprocessor.pkl                # Preprocessing pipeline
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Exploratory Data Analysis (EDA)

The notebook begins with an extensive EDA to understand:

* Churn vs non-churn distributions
* Feature relationships with churn
* Data imbalance and skewness
* Potential data leakage risks
* Redundant and noisy features

Key insights from EDA directly informed:

* Feature selection
* Preprocessing strategy
* Model choice

> **Takeaway:** Strong models start with strong data understanding.

---

## ğŸ§© Feature Engineering & Preprocessing

A robust preprocessing pipeline was built using **scikit-learnâ€™s ColumnTransformer**:

* **Categorical features**

  * OneHotEncoding
* **Numerical features**

  * QuantileTransformer (to handle skew and outliers)

All preprocessing steps are:

* Fit only on training data
* Serialized and reused during inference to prevent leakage

---

## ğŸ¤– Model Development

Multiple models were trained and compared:

| Model               | Purpose                      |
| ------------------- | ---------------------------- |
| Logistic Regression | Baseline + interpretability  |
| Random Forest       | Non-linear interactions      |
| XGBoost             | Final high-performance model |

### Hyperparameter Tuning

* RandomizedSearchCV
* HalvingRandomSearchCV

### Evaluation Metrics

* ROC-AUC
* Precision / Recall
* F1-Score

**Final Model Performance**

* ROC-AUC â‰ˆ **0.95**

---

## ğŸ” Model Interpretability

Interpretability was treated as a first-class concern:

* Logistic Regression coefficients
* Tree-based feature importance
* **SHAP** for:

  * Global explanations
  * Local, per-prediction explanations

This ensures model decisions are **transparent and explainable**, which is critical for business adoption.

---

## âš™ï¸ FastAPI Inference Service

The trained model is deployed as a **FastAPI application**:

### Key Features

* Strict request validation using schemas
* Low-latency inference endpoint
* Per-request latency measurement
* Robust error handling

### Sample Endpoint

```
POST /predict
```

Returns:

* Churn probability
* Class prediction

---

## ğŸ“ Prediction Logging (Observability)

Every inference request is logged using **SQLAlchemy ORM** into SQLite.

Each log captures:

* Timestamp
* Input features
* Predicted probability & class
* Latency (ms)
* Error (if any)
* Model version

This enables:

* Traceability
* Debugging
* Monitoring
* Auditing

---

## ğŸ“ˆ Monitoring Dashboard (Streamlit)

A Streamlit dashboard provides **real-time observability**:

### Metrics Tracked

* Total predictions
* Churn rate
* Latency trends
* Error rates
* High-risk customers
* Time-based comparisons

---

## ğŸ“‰ Model Drift Detection

To monitor post-deployment behavior, **Population Stability Index (PSI)** is used on predicted probabilities.

### Why PSI?

* Works without immediate labels
* Industry-standard (finance, risk modeling)
* Interpretable thresholds

| PSI Value  | Interpretation                      |
| ---------- | ----------------------------------- |
| < 0.1      | No drift                            |
| 0.1 â€“ 0.25 | Moderate drift                      |
| > 0.25     | High drift (retraining recommended) |

This provides early warning signals when model behavior changes.

---

## ğŸ§ª Demo 

To demo the system :

```bash

streamlit run dashboard.py
streamlit run demo.py
uvicorn api.main:app --reload
```
* Run db.py and models.py to ensure the database is created and the table is available
* Run the fastapi app next with uvicorn,if using powershell ensure you are in the respective api directory before running.
* Run the dashboard and demo streamlit apps to view the logging and get prediction


---

## ğŸ› ï¸ Tech Stack

* **Python**
* **pandas, NumPy**
* **scikit-learn**
* **XGBoost**
* **SHAP**
* **FastAPI**
* **SQLAlchemy**
* **SQLite**
* **Streamlit**

---

## ğŸ¯ Key Learnings

* Model accuracy alone is not sufficient
* Observability is critical for ML systems
* Drift detection is essential post-deployment
* End-to-end ownership matters more than isolated models
* Production ML requires both data science and engineering thinking

---

## ğŸš€ Future Improvements

* Feature-level drift detection
* Automated retraining pipelines
* Alerting for SLA breaches
* Cloud deployment
* A/B model comparison (v1 vs v2)

---

## ğŸ“¬ Contact

If youâ€™d like to discuss this project or provide feedback, feel free to reach out via LinkedIn or GitHub.

